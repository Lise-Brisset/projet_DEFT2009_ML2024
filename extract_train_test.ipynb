{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________\n",
    "_Notebook créé par Lise Brisset, le 21/12/2024, dans le cadre du cours de d'Apprentissage Artificiel (enseignant : Loïc Grobol)._\n",
    "\n",
    "Groupe du projet ML2024 DEFT09 : Patricia Augustyn, Lise Brisset et Solomiia Korol.\n",
    "\n",
    "Ce notebook permet d'extraire les données de train et de test du corpus DEFT09 et de les enregistrer en format JSON.\n",
    "\n",
    "Il s'agit des données recueillies et transcrites du parlement européen entre 1999 et 2004. \n",
    "\n",
    "___________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des librairies nécessaires à ce programme : \n",
    "import xml.etree.ElementTree as ET\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des données de train en français :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On récupère les données de train en parcourant le fichier xml correspondant : \n",
    "\n",
    "Lien doc etree : https://docs.python.org/3/library/xml.etree.elementtree.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on importe le fichier xml des données de train : \n",
    "with open('/home/lise/Documents/TAL_M2S3/apprentissage_artificiel/projet_en_groupe/deft09/Corpus d_apprentissage/deft09_parlement_appr_fr.xml', 'r') as xml_file:\n",
    "    xml_tree = ET.parse(xml_file)\n",
    "    root = xml_tree.getroot()\n",
    "\n",
    "# Listes pour stocker les résultats\n",
    "liste_train = []\n",
    "\n",
    "# On récupère tous les documents :\n",
    "for doc in root.findall('doc'):\n",
    "    # On récupère l'attribut \"id\" du doc : \n",
    "    id_doc = doc.attrib['id']\n",
    "    \n",
    "    # On récupère le parti politique\n",
    "    parti = None\n",
    "    eval_parti = doc.find('.//EVAL_PARTI/PARTI')  # Utilise XPath pour trouver le parti\n",
    "    if eval_parti is not None:\n",
    "        parti = eval_parti.attrib['valeur']  # Récupère la valeur de l'attribut \"valeur\"\n",
    "    \n",
    "    # On récupère le texte :\n",
    "    texte = []  # Initialise une liste pour stocker les paragraphes\n",
    "    texte_element = doc.find('texte')  # Trouve l'élément <texte>\n",
    "    if texte_element is not None:\n",
    "        for simple_p in texte_element.findall('p'):  # Récupère tous les <p>\n",
    "            if simple_p.text and simple_p.text.strip():  # Vérifie que le texte n'est pas vide\n",
    "                texte.append(simple_p.text.strip())  # Ajoute le texte à la liste\n",
    "\n",
    "    # On concatène les paragraphes du texte :\n",
    "    texte_concatene = ' '.join(texte)  # Concatène les paragraphes\n",
    "\n",
    "    # Ajoute les résultats à la liste\n",
    "    liste_train.append({\n",
    "        'id': id_doc,\n",
    "        'parti': parti,\n",
    "        'texte': texte_concatene\n",
    "    })\n",
    "\n",
    "# Afficher quelques résultats pour vérifier : \n",
    "for i in range(5):\n",
    "    print(f\"id : {liste_train[i]['id']}, parti : {liste_train[i]['parti']}, texte : {liste_train[i]['texte']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Et on enregistre au format JSON, en faisant attention à l'encodage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on ouvre le fichier json en écriture :\n",
    "with open('/home/lise/Documents/TAL_M2S3/apprentissage_artificiel/projet_en_groupe/data_parlement/train/deft09_parlement_train.json', 'w') as json_file:\n",
    "    # avec train qui englobe tout le contenu de la liste_train: \n",
    "    # ensure_ascii=False pour éviter les problèmes d'encodage\n",
    "    json.dump({'train': liste_train}, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Fichier JSON créé avec succès dans le dossier data_parlement/train/ !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Même chose avec les données de test : \n",
    "\n",
    "Cette fois-ci nous devons récupérer les références des partis politiques en fonction des ids de chaque doc du fichier xml de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On récupère déjà les référence qu'on stocke dans un dictionnaire : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire pour stocker les données\n",
    "donnees_dict = {}\n",
    "\n",
    "# Récupèrons les références dans le fichier des références de test : \n",
    "with open('/home/lise/Documents/TAL_M2S3/apprentissage_artificiel/projet_en_groupe/deft09/Données de référence/deft09_parlement_ref_fr.txt', 'r') as ref_file:\n",
    "    # les id et les ref sont séparés par une tabulation : \n",
    "    for ligne in ref_file:\n",
    "        colonnes = ligne.split('\\t')\n",
    "        id_ref = colonnes[0]  # La première colonne est l'ID\n",
    "        classe = colonnes[1].replace(\"\\n\", \"\")  # La deuxième colonne est la classe\n",
    "        donnees_dict[id_ref] = classe  # Ajoute l'ID et la classe au dictionnaire\n",
    "\n",
    "# Afficher le dictionnaire\n",
    "print(donnees_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Puis on parcourt le fichier xml des données de test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus de test :\n",
    "with open('/home/lise/Documents/TAL_M2S3/apprentissage_artificiel/projet_en_groupe/deft09/Corpus de test/deft09_parlement_test_fr.xml', 'r') as xml_file:\n",
    "    xml_tree = ET.parse(xml_file)\n",
    "    root = xml_tree.getroot()\n",
    "\n",
    "# Listes pour stocker les résultats\n",
    "liste_test = []\n",
    "\n",
    "# On récupère tous les documents :\n",
    "for doc in root.findall('doc'):\n",
    "    # On récupère l'attribut \"id\" du doc : \n",
    "    id_doc = doc.attrib['id']\n",
    "\n",
    "    # on récupère le parti politique en fonction de l'id :\n",
    "    parti = donnees_dict[id_doc]\n",
    "    \n",
    "    # On récupère le texte :\n",
    "    texte = []  # Initialise une liste pour stocker les paragraphes\n",
    "    texte_element = doc.find('texte')  # Trouve l'élément <texte>\n",
    "    if texte_element is not None:\n",
    "        for simple_p in texte_element.findall('p'):  # Récupère tous les <p>\n",
    "            if simple_p.text and simple_p.text.strip():  # Vérifie que le texte n'est pas vide\n",
    "                texte.append(simple_p.text.strip())  # Ajoute le texte à la liste\n",
    "\n",
    "    # On concatène les paragraphes du texte :\n",
    "    texte_concatene = ' '.join(texte)  # Concatène les paragraphes\n",
    "\n",
    "    # Ajoute les résultats à la liste\n",
    "    liste_test.append({\n",
    "        'id': id_doc,\n",
    "        'parti': parti,\n",
    "        'texte': texte_concatene\n",
    "    })\n",
    "\n",
    "# Afficher quelques résultats pour vérifier :\n",
    "for i in range(5):\n",
    "    print(f\"id : {liste_test[i]['id']}, parti : {liste_test[i]['parti']}, texte : {liste_test[i]['texte']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Et on enregistre le tout dans un fichier JSON : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on ouvre le fichier json en écriture :\n",
    "with open('/home/lise/Documents/TAL_M2S3/apprentissage_artificiel/projet_en_groupe/data_parlement/test/deft09_parlement_test.json', 'w') as json_file:\n",
    "    # avec test qui englobe tout le contenu de la liste_test:\n",
    "    # ensure_ascii=False pour éviter les problèmes d'encodage\n",
    "    json.dump({'test': liste_test}, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Fichier JSON créé avec succès dans le dossier data_parlement/test/ !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_apprentissage_auto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
